---
title: '7. Moving Beyond Linearity'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(ISLR)
library(boot)
library(gam)
library(MASS)
library(splines)
library(leaps)
```


##6
###a use cross-validation to select the optimal degree d for the polynomial regression
```{r}
set.seed(123)
data(Wage)

#use 10-fold cross-validation here
cv_error = rep(0,10)
for (i in 1:10) {
  glm.fit = glm(wage ~ poly(age,i), data = Wage)
  cv_error[i] = cv.glm(Wage, glm.fit, K=10)$delta[1]  
}

plot(cv_error, type="b")  

#according to the plot, I'd choose d = 4
```

###a ANOVA
```{r}
fit.01 = lm(wage ~ age, data = Wage)
fit.02 = lm(wage ~ poly(age,2), data = Wage)
fit.03 = lm(wage ~ poly(age,3), data = Wage)
fit.04 = lm(wage ~ poly(age,4), data = Wage)
fit.05 = lm(wage ~ poly(age,5), data = Wage)
fit.06 = lm(wage ~ poly(age,6), data = Wage)
fit.07 = lm(wage ~ poly(age,7), data = Wage)
fit.08 = lm(wage ~ poly(age,8), data = Wage)
fit.09 = lm(wage ~ poly(age,9), data = Wage)
fit.10 = lm(wage ~ poly(age,10), data = Wage)
anova(fit.01,fit.02,fit.03,fit.04,fit.05,fit.06,fit.07,fit.08,fit.09,fit.10)
# Based on ANOVA test, the cubic (3th degree) appear to provide a reasonable fit to the data. But 4th degree 
```

```{r}
# use d=3 to fit a polynomial regression
agelims = range(Wage$age)
age.grid = seq(agelims[1], agelims[2])
preds = predict(fit.03, newdata = list(age = age.grid), se = TRUE)
se.bands = preds$fit + cbind(2 * preds$se.fit, -2 * preds$se.fit)
```

```{r}
#plot the data and add the fit from the degree-3 polynomial
par(mfrow = c(1,1), mar = c(4.5,4.5,1,1), oma = c(0,0,4,0))
plot(Wage$age, Wage$wage, xlim = agelims, cex = 0.5, col = "darkgrey")
title("Degree 3 Polynomial Fit", outer = TRUE)
lines(age.grid, preds$fit, lwd = 2, col = "blue")
matlines(age.grid, se.bands, lwd = 1, col = "blue", lty = 3)
```

###b Fit a step function
```{r}
cv_error = rep(0,9)
for (i in 2:10) {
  Wage$age.cut = cut(Wage$age,i)
  glm.fit = glm(wage~age.cut, data = Wage)
  cv_error[i-1] = cv.glm(Wage, glm.fit, K = 10)$delta[1]  
}
plot(2:10, cv_error, type="b")  
# 8 cut appear to be optimal
```
```{r}
cut.fit = glm(wage ~ cut(age,8), data = Wage)
preds = predict(cut.fit, newdata = list(age=age.grid), se = TRUE)
se.bands = preds$fit + cbind(2 * preds$se.fit, -2 * preds$se.fit)
```

```{r}
#plot
par(mfrow = c(1,1), mar = c(4.5,4.5,1,1), oma = c(0,0,4,0))
plot(Wage$age, Wage$wage, xlim = agelims, cex = 0.5, col = "darkgrey")
title("Fit with 8 Age Bands")
lines(age.grid, preds$fit, lwd = 2, col = "blue")
matlines(age.grid, se.bands, lwd = 1, col = "blue", lty = 3)
```

##7
```{r}
#plot the relationship with variables with Wage
plot(Wage$maritl, Wage$wage)
plot(Wage$jobclass, Wage$wage)
```

```{r}
#fit a GAM model
gam.m1 = gam(wage ~ ns(age,5), data = Wage)
gam.m2 = gam(wage ~ ns(age,5) + maritl, data = Wage)
gam.m3 = gam(wage ~ ns(age,5) + jobclass, data = Wage)
gam.m4 = gam(wage ~ ns(age,5) + maritl+ jobclass, data = Wage)
anova(gam.m1, gam.m2, gam.m4)
anova(gam.m1, gam.m3, gam.m4)
```

```{r}
# plot the result
par(mfrow = c(1,3))
plot(gam.m4, se = TRUE, col = "blue")
```

##8
```{r}
#try to find the non-linear relationship with mpg
data(Auto)
pairs(Auto)
#displacement, horsepower, weight, acceleration may have nonlinear relationships
```

```{r}
# Polynomial fit on weight
cv_error = rep(0,10)
for (i in 1:10) {
  glm.fit = glm(mpg ~ poly(weight,i), data = Auto)
  cv_error[i] = cv.glm(Auto, glm.fit, K = 10)$delta[1]
}
plot(cv_error, type = "b")  
#d=2 appear to be optimal
```

```{r}
# GAM fit
gam.m1 = gam(mpg ~ poly(weight,2), data = Auto)
gam.m2 = gam(mpg ~ poly(weight,2) + displacement, data = Auto)
gam.m3 = gam(mpg ~ poly(weight,2) + acceleration, data = Auto)
gam.m4 = gam(mpg ~ poly(weight,2) + displacement + acceleration, data = Auto)
anova(gam.m1, gam.m2, gam.m4)
anova(gam.m1, gam.m3, gam.m4)
```

```{r}
#plot the result
par(mfrow = c(1,3))
plot(gam.m4, se = TRUE, col = "blue")
```

##9
###a fit a cubic polynomial regression
```{r}
data(Boston)

fit = lm(nox ~ poly(dis,3), data = Boston)
dislims = range(Boston$dis)
dis.grid = seq(dislims[1], dislims[2], 0.1)
preds = predict(fit, newdata = list(dis = dis.grid), se = TRUE)
se.bands = preds$fit + cbind(2 * preds$se.fit, -2 * preds$se.fit)
summary(fit)
```

```{r}
#plot the data and add the fit from the degree-3 polynomial
par(mfrow = c(1,1), mar = c(4.5,4.5,1,1), oma = c(0,0,4,0))
plot(Boston$dis, Boston$nox, xlim=dislims, cex = 0.5, col = "darkgrey")
title("Degree 3 Polynomial Fit", outer = TRUE)
lines(dis.grid, preds$fit, lwd = 2, col = "blue")
matlines(dis.grid, se.bands, lwd = 1, col = "blue", lty = 3)
```

###b 
```{r}
rss = rep(0,10)
for (i in 1:10) {
  lm.fit = lm(nox ~ poly(dis,i), data = Boston)
  rss[i] = sum(lm.fit$residuals^2)
}
plot(rss, type="b")  
```

###c perform cross-validation to select degree
```{r}
#use 10-fold cross-validation
cv_error = rep(0,10)
for (i in 1:10) {
  glm.fit = glm(nox ~ poly(dis,i), data = Boston)
  cv_error[i] = cv.glm(Boston, glm.fit, K = 10)$delta[1]
}
plot(cv_error, type="b")
#d=4 appear to be optimal
```

###d fit a regression spline, using four degree of freedom
```{r}
fit = lm(nox ~ bs(dis, df = 4), data = Boston)
pred = predict(fit, newdata = list(dis = dis.grid), se = T)
#plot
plot(Boston$dis, Boston$nox, col="gray")
lines(dis.grid, pred$fit, lwd = 2)
lines(dis.grid, pred$fit + 2*pred$se, lty = "dashed")
lines(dis.grid, pred$fit - 2*pred$se, lty = "dashed")
#choose knots
attr(bs(Boston$dis,df=4),"knots")
```

###e fit a regression spline for a range of degree of freedom
```{r}
rss = rep(0,10)
for (i in 4:13) {
  fit = lm(nox ~ bs(dis, df = i), data = Boston)
  rss[i-3] <- sum(fit$residuals^2)
}
plot(4:13, rss, type="b")
```

###f perform cross-validation to select the best degree of freedom
```{r}
#use 10-fold cross-validation
cv_error = rep(0,10)
for (i in 4:13) {
  glm.fit = glm(nox ~ bs(dis, df = i), data = Boston)
  cv_error[i-3] = cv.glm(Boston, glm.fit, K = 10)$delta[1]
}
plot(4:13, cv_error, type="b")
#d = 5 appear to be optimal
```

##10
###a 
```{r}
#train-test split
data(College)
trainid = sample(1:nrow(College), nrow(College)/2)
train = College[trainid,]
test = College[-trainid,]
```

```{r}
#create a predict function
predict.regsubsets = function(object, newdata, id, ...){
  form = as.formula(object$call[[2]])
  mat = model.matrix(form, newdata)
  coefi = coef(object, id = id)
  xvars = names(coefi)
  mat[,xvars]%*%coefi
}

#perform forward selection
fit.fwd = regsubsets(Outstate~., data = train, nvmax = ncol(College) - 1)
fwd.summary = summary(fit.fwd)
err_fwd = rep(NA, ncol(College) - 1)
for(i in 1:(ncol(College) - 1)) {
  pred_fwd = predict(fit.fwd, test, id = i)
  err_fwd[i] = mean((test$Outstate - pred_fwd)^2)
}
```

```{r}
#plot
par(mfrow = c(2,2))
plot(err_fwd, type = "b", main = "Test MSE", xlab = "Number of Predictors")
mse_min = which.min(err_fwd)  
points(mse_min, err_fwd[min.mse], col = "red", pch = 4, lwd = 5)
```

```{r}
plot(fwd.summary$adjr2, type = "b", main = "Adjusted R^2", xlab = "Number of Predictors")
adjr2_max = which.max(fwd.summary$adjr2)  
points(adjr2_max, fwd.summary$adjr2[adjr2_max], col = "red", pch = 4, lwd = 5)
```

```{r}
plot(fwd.summary$cp, type = "b", main = "cp", xlab = "Number of Predictors")
cp_min = which.min(fwd.summary$cp)  
points(cp_min, fwd.summary$cp[cp_min], col = "red", pch = 4, lwd = 5)

```

```{r}
plot(fwd.summary$bic, type = "b", main = "bic", xlab = "Number of Predictors")
bic_min = which.min(fwd.summary$bic)  
points(bic_min, fwd.summary$bic[bic_min], col = "red", pch = 4, lwd = 5)
```

```{r}
# 6 predictors appear to be optimal
coef(fit.fwd, 6)
```

###b fit a GAM model
```{r}
gam.fit = gam(Outstate ~ Private + s(Room.Board,3) + s(Terminal,3) + s(perc.alumni,3) + s(Expend,3) + s(Grad.Rate,3), data = College)
```

```{r}
#plot the result
par(mfrow = c(2,3))
plot(gam.fit, se = TRUE, col = "blue")
```

###c Evaluate the model on the test set
```{r}
pred = predict(gam.fit, test)
mse= mean((test$Outstate - pred)^2)
mse
```

###d
```{r}
summary(gam.fit)
```

##11
###a
```{r}
X1 = rnorm(100)
X2 = rnorm(100)
b0 = -5
b1 = 4
b2 = 2
eps = rnorm(100, sd = 1)
Y = b0 + b1 * X1 + b2 * X2 + eps
```

###b
```{r}
beta1 = 2
```

###c
```{r}
a = Y - beta1 * X1
beta2 = lm(a ~ X2)$coef[2]
```

###d
```{r}
a = Y - beta2 * X2
beta1 = lm(a ~ X1)$coef[2]
```

###e
```{r}
beta0 = rep(0, 1000)
beta1 = rep(0, 1000)
beta2 = rep(0, 1000)

for (i in 1:1000) {
  a = Y - beta0 [i] * X1
  beta2[i] = lm(a ~ X2)$coef[2]
  a = Y - beta2[i] * X2
  beta0[i] = lm(a ~ X1)$coef[1]
  beta1[i+1] <- lm(a ~ X1)$coef[2]
}
```
