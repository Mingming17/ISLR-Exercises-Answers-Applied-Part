---
title: "6. Linear Model Selection and Regularization"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(ISLR)
library(leaps)
library(glmnet)
library(pls)
```

##8
###a
```{r}
set.seed(123)
X = rnorm(100)
e = rnorm(100, mean = 0,sd = 0.1)
```

###b
```{r}
b0 = 10
b1 = 8
b2 = 3
b3 = 5
Y = b0 + b1*X + b2*X^2 + b3*X^3 + e
```

###C Best Subset Selection
```{r}
df = data.frame(Y,X,X^2,X^3,X^4,X^5,X^6,X^7,X^8,X^9,X^10)
```

```{r}
regfit.full = regsubsets(Y~.,df)
reg_summary = summary(regfit.full,nvmax = 10)
reg_summary$cp
reg_summary$bic
reg_summary$adjr2
```

```{r}
plot(reg_summary$cp,type = "l")
plot(reg_summary$bic,type = "l")
plot(reg_summary$adjr2,type = "l")
```

The three variable model can be the best chioce.

###d Forward Stepwise Selection
```{r}
regfit.forward = regsubsets(Y~.,df,nvmax = 10, method = 'forward')
fwd_summary = summary(regfit.forward)
fwd_summary$cp
fwd_summary$bic
fwd_summary$adjr2
```

###d Backward Stepwise Selection
```{r}
regfit.backward = regsubsets(Y~.,df,nvmax = 10, method = 'backward')
bwd_summary = summary(regfit.backward)
bwd_summary$cp
bwd_summary$bic
bwd_summary$adjr2
```

Results are simillar to the best subset selection.

###e Lasso 
```{r}
x = model.matrix(Y~.,df)[,-1]
y = df$Y
train = sample(1:100,70)
test = -train
```

```{r}
cv.out = cv.glmnet(x[train,],y[train],alpha = 1)
cv.out$lambda.min
plot(cv.out)
lasso = glmnet(x,y,alpha = 1)
predict(lasso,type = "coefficients",s=0.5)[1:11,]
```

###f
```{r}
b7 = 2
Y2 = b0 + b7*X^7 + e
df2 = data.frame(Y2,X,X^2,X^3,X^4,X^5,X^6,X^7,X^8,X^9,X^10)
```

###Best Subset Selection
```{r}
regfit.full2 = regsubsets(Y2~.,df2)
reg_summary2 = summary(regfit.full2,nvmax = 10)
reg_summary2$cp
reg_summary2$bic
reg_summary2$adjr2
```

###Lasso
```{r}
x2 = model.matrix(Y2~.,df2)[,-1]
y2 = df2$Y2
train2 = sample(1:100,70)
test2 = -train2
```

```{r}
cv.out2 = cv.glmnet(x2[train2,],y2[train2],alpha = 1)
cv.out2$lambda.min
plot(cv.out2)
lasso2 = glmnet(x2,y2,alpha = 1)
predict(lasso2,type = "coefficients",s=2)[1:11,]
```

##9
###a train-test data split
```{r}
set.seed(123)
x = model.matrix(Apps~.,College)[,-1]
y = College$Apps
train = sample(1:nrow(x),nrow(x)/1.4)
test = -train
```

###b linear model
```{r}
lm = lm(Apps~., data = data.frame(College[train,]))
lm_pred = predict(lm, data.frame(College[test,]),type = "response")
```

###c ridge regression
```{r}
cv.out = cv.glmnet(x[train,],y[train],alpha = 0)
bestlam = cv.out$lambda.min
ridge = glmnet(x[train,],y[train],alpha = 0,lamda = grid,thresh = 1e-12)
ridge_pred = predict(ridge,s = bestlam, newx = x[test,])
mean((ridge_pred - y[test])^2)
```

###d lasso
```{r}
cv.out = cv.glmnet(x[train,],y[train],alpha = 1)
bestlam = cv.out$lambda.min
lasso = glmnet(x[train,],y[train],alpha = 1,lamda = grid)
lasso_pred = predict(lasso,s = bestlam, newx = x[test,])
mean((lasso_pred - y[test])^2)
lasso_coef = predict(lasso,type = "coefficients", s = bestlam)[1:18,]
lasso_coef
```

###e PCR
```{r}
pcr = pcr(Apps~.,data = College, subset = train, scale = T, validation = "CV")
validationplot(pcr, val.type = "MSEP")
```

```{r}
pcr_pred = predict(pcr,x[test,],ncomp = 5)
mean((pcr_pred - y[test])^2)
```

###f PLS
```{r}
pls = plsr(Apps~.,data = College, subset = train, scale = T, validation = "CV")
validationplot(pls, val.type = "MSEP")
```

```{r}
pls_pred = predict(pls,x[test,],ncomp = 5)
mean((pls_pred - y[test])^2)
```


##11
###train test data split
```{r}
set.seed(123)
summary(Boston)
trainid = sample(1:nrow(Boston), nrow(Boston)/2)
train = Boston[trainid,]
test = Boston[-trainid,]
train_matrix = model.matrix(crim~., data=train)[,-1]
test_matrix = model.matrix(crim~., data=test)[,-1]
```

###ridge regression
```{r}
cv.out = cv.glmnet(train_matrix,test_matrix,alpha = 0)
bestlam = cv.out$lambda.min
ridge = glmnet(train_matrix,test_matrix,alpha = 0,lamda = grid)
ridge_pred = predict(ridge,s = bestlam, newx = test_matrix)
mean((ridge_pred - test$crim)^2)
```

###lasso regression
```{r}
cv.out = cv.glmnet(train_matrix,test_matrix,alpha = 1)
bestlam = cv.out$lambda.min
lasso = glmnet(train_matrix,test_matrix,alpha = 1,lamda = grid)
lasso_pred = predict(lasso,s = bestlam, newx = test_matrix)
mean((lasso_pred - test$crim)^2)
```

###forward selection
```{r}
forward = regsubsets(crim~., data=train, nvmax=ncol(Boston)-1,method = 'forward')
fwd_summary = summary(forward)
fwd_summary$cp
fwd_summary$bic
fwd_summary$adjr2
```

```{r}
fwd_error = rep(NA, ncol(Boston)-1)
for(i in 1:(ncol(Boston)-1)) {
  fwd_pred <- predict(forward, test, id=i)
  fwd_error[i] <- mean((test$crim - fwd_pred)^2)
}
plot(fwd_error, type="b", main="Test MSE for Forward Selection", xlab="Number of Predictors")
which.min(fwd_error)
```

###backward selection
```{r}
backward <- regsubsets(crim~., data=train, nvmax=ncol(Boston)-1,method = 'backward')
bwd_summary = summary(backward)
bwd_summary$cp
bwd_summary$bic
bwd_summary$adjr2
```

```{r}
bwd_error = rep(NA, ncol(Boston)-1)
for(i in 1:(ncol(Boston)-1)) {
  bwd_pred <- predict(backward, test, id=i)
  bwd_error[i] <- mean((test$crim - bwd_pred)^2)
}
plot(bwd_error, type="b", main="Test MSE for Forward Selection", xlab="Number of Predictors")
which.min(bwd_error)
```

```{r}
cp_min = which.min(fwd_summary$cp)  
plot(fwd_summary$cp, xlab="Number of Poly(X)", ylab="Forward Selection Cp", type="l")
points(cp_min, fwd_summary$cp[cp_min], col="red", pch=4, lwd=5)

cp_min = which.min(bwd_summary$cp)  
plot(bwd_summary$cp, xlab="Number of Poly(X)", ylab="=Backward Selection Cp", type="l")
points(cp_min, bwd_summary$cp[cp_min], col="red", pch=4, lwd=5)
```

```{r}
bic_min = which.min(fwd_summary$bic)  
plot(fwd_summary$bic, xlab="Number of Poly(X)", ylab="Forward Selection BIC", type="l")
points(bic_min, fwd_summary$bic[bic_min], col="red", pch=4, lwd=5)

bic_min = which.min(bwd_summary$bic)  
plot(bwd_summary$bic, xlab="Number of Poly(X)", ylab="Backward Selection BIC", type="l")
points(bic_min, bwd_summary$bic[bic_min], col="red", pch=4, lwd=5)
```

```{r}
adjr2_min = which.min(fwd_summary$adjr2)  
plot(fwd_summary$adjr2, xlab="Number of Poly(X)", ylab="Forward Selection Adjusted R^2", type="l")
points(adjr2_min, fwd_summary$adjr2[adjr2_min], col="red", pch=4, lwd=5)

adjr2_min = which.min(bwd_summary$adjr2)  
plot(bwd_summary$adjr2, xlab="Number of Poly(X)", ylab="Backward Selection Adjusted R^2", type="l")
points(adjr2_min, bwd_summary$adjr2[adjr2_min], col="red", pch=4, lwd=5)
```

###I would choose the Lasso Regression Model.