---
title: "6. Linear Model Selection and Regularization"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(ISLR)
library(leaps)
library(glmnet)
library(pls)
```

##8
###a
```{r}
set.seed(123)
X = rnorm(100)
e = rnorm(100, mean = 0,sd = 0.1)
```

###b
```{r}
b0 = 10
b1 = 8
b2 = 3
b3 = 5
Y = b0 + b1*X + b2*X^2 + b3*X^3 + e
```

###C Best Subset Selection
```{r}
df = data.frame(Y,X,X^2,X^3,X^4,X^5,X^6,X^7,X^8,X^9,X^10)
```

```{r}
regfit.full = regsubsets(Y~.,df)
reg_summary = summary(regfit.full,nvmax = 10)
reg_summary$cp
reg_summary$bic
reg_summary$adjr2
```

```{r}
plot(reg_summary$cp,type = "l")
plot(reg_summary$bic,type = "l")
plot(reg_summary$adjr2,type = "l")
```

The three variable model can be the best chioce.

###d Forward Stepwise Selection
```{r}
regfit.forward = regsubsets(Y~.,df,nvmax = 10, method = 'forward')
fwd_summary = summary(regfit.forward)
fwd_summary$cp
fwd_summary$bic
fwd_summary$adjr2
```

###d Backward Stepwise Selection
```{r}
regfit.backward = regsubsets(Y~.,df,nvmax = 10, method = 'backward')
bwd_summary = summary(regfit.backward)
bwd_summary$cp
bwd_summary$bic
bwd_summary$adjr2
```

Results are simillar to the best subset selection.

###e Lasso 
```{r}
x = model.matrix(Y~.,df)[,-1]
y = df$Y
train = sample(1:100,70)
test = -train
```

```{r}
cv.out = cv.glmnet(x[train,],y[train],alpha = 1)
cv.out$lambda.min
plot(cv.out)
lasso = glmnet(x,y,alpha = 1)
predict(lasso,type = "coefficients",s=0.5)[1:11,]
```

###f
```{r}
b7 = 2
Y2 = b0 + b7*X^7 + e
df2 = data.frame(Y2,X,X^2,X^3,X^4,X^5,X^6,X^7,X^8,X^9,X^10)
```

###Best Subset Selection
```{r}
regfit.full2 = regsubsets(Y2~.,df2)
reg_summary2 = summary(regfit.full2,nvmax = 10)
reg_summary2$cp
reg_summary2$bic
reg_summary2$adjr2
```

###Lasso
```{r}
x2 = model.matrix(Y2~.,df2)[,-1]
y2 = df2$Y2
train2 = sample(1:100,70)
test2 = -train2
```

```{r}
cv.out2 = cv.glmnet(x2[train2,],y2[train2],alpha = 1)
cv.out2$lambda.min
plot(cv.out2)
lasso2 = glmnet(x2,y2,alpha = 1)
predict(lasso2,type = "coefficients",s=2)[1:11,]
```

##9
###a train-test data split
```{r}
set.seed(123)
x = model.matrix(Apps~.,College)[,-1]
y = College$Apps
train = sample(1:nrow(x),nrow(x)/1.4)
test = -train
```

###b linear model
```{r}
lm = lm(Apps~., data = data.frame(College[train,]))
lm_pred = predict(lm, data.frame(College[test,]),type = "response")
```

###c ridge regression
```{r}
cv.out = cv.glmnet(x[train,],y[train],alpha = 0)
bestlam = cv.out$lambda.min
ridge = glmnet(x[train,],y[train],alpha = 0,lamda = grid,thresh = 1e-12)
ridge_pred = predict(ridge,s = bestlam, newx = x[test,])
mean((ridge_pred - y[test])^2)
```

###d lasso
```{r}
cv.out = cv.glmnet(x[train,],y[train],alpha = 1)
bestlam = cv.out$lambda.min
lasso = glmnet(x[train,],y[train],alpha = 1,lamda = grid)
lasso_pred = predict(lasso,s = bestlam, newx = x[test,])
mean((lasso_pred - y[test])^2)
lasso_coef = predict(lasso,type = "coefficients", s = bestlam)[1:18,]
lasso_coef
```

###e PCR
```{r}
pcr = pcr(Apps~.,data = College, subset = train, scale = T, validation = "CV")
validationplot(pcr, val.type = "MSEP")
```

```{r}
pcr_pred = predict(pcr,x[test,],ncomp = 5)
mean((pcr_pred - y[test])^2)
```

###f PLS
```{r}
pls = plsr(Apps~.,data = College, subset = train, scale = T, validation = "CV")
validationplot(pls, val.type = "MSEP")
```

```{r}
pls_pred = predict(pls,x[test,],ncomp = 5)
mean((pls_pred - y[test])^2)
```
